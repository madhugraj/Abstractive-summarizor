{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [Seq2seq learning](https://arxiv.org/abs/1409.3215)\n",
    "with [attention mechanism](https://arxiv.org/abs/1409.0473), specifically [local attention](https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-processed Dataset\n",
    "\n",
    "The Data is preprocessed in [Data_Pre-Processing.ipynb](https://github.com/JRC1995/Abstractive-Summarization/blob/master/Data_Pre-Processing.ipynb)\n",
    "\n",
    "Dataset source: https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('Processed_Data/Amazon_Reviews_Processed.json') as file:\n",
    "\n",
    "    for json_data in file:\n",
    "        saved_data = json.loads(json_data)\n",
    "\n",
    "        vocab2idx = saved_data[\"vocab\"]\n",
    "        embd = saved_data[\"embd\"]\n",
    "        train_batches_text = saved_data[\"train_batches_text\"]\n",
    "        test_batches_text = saved_data[\"test_batches_text\"]\n",
    "        val_batches_text = saved_data[\"val_batches_text\"]\n",
    "        train_batches_summary = saved_data[\"train_batches_summary\"]\n",
    "        test_batches_summary = saved_data[\"test_batches_summary\"]\n",
    "        val_batches_summary = saved_data[\"val_batches_summary\"]\n",
    "        train_batches_true_text_len = saved_data[\"train_batches_true_text_len\"]\n",
    "        val_batches_true_text_len = saved_data[\"val_batches_true_text_len\"]\n",
    "        test_batches_true_text_len = saved_data[\"test_batches_true_text_len\"]\n",
    "        train_batches_true_summary_len = saved_data[\"train_batches_true_summary_len\"]\n",
    "        val_batches_true_summary_len = saved_data[\"val_batches_true_summary_len\"]\n",
    "        test_batches_true_summary_len = saved_data[\"test_batches_true_summary_len\"]\n",
    "\n",
    "        break\n",
    "        \n",
    "idx2vocab = {v:k for k,v in vocab2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "max_summary_len = 31 # should be summary_max_len as used in data_preprocessing with +1 (+1 for <EOS>) \n",
    "D = 5 # D determines local attention window size\n",
    "window_len = 2*D+1\n",
    "l2=1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow.compat.v1 as tf \n",
    "import tensorflow as tf\n",
    "\n",
    "#tf.disable_v2_behavior()\n",
    "#tf.disable_eager_execution()\n",
    "\n",
    "embd_dim = len(embd[0])\n",
    "\n",
    "tf_text = tf.placeholder(tf.int32, [None, None])\n",
    "tf_embd = tf.placeholder(tf.float32, [len(vocab2idx),embd_dim])\n",
    "tf_true_summary_len = tf.placeholder(tf.int32, [None])\n",
    "tf_summary = tf.placeholder(tf.int32,[None, None])\n",
    "tf_train = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x,rate,training):\n",
    "    return tf.cond(tf_train,\n",
    "                    lambda: tf.nn.dropout(x,rate=0.3),\n",
    "                    lambda: x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed vectorized text\n",
    "\n",
    "Dropout used for regularization \n",
    "(https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_text = tf.nn.embedding_lookup(tf_embd, tf_text)\n",
    "\n",
    "embd_text = dropout(embd_text,rate=0.3,training=tf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM function\n",
    "\n",
    "More info: \n",
    "<br>\n",
    "https://dl.acm.org/citation.cfm?id=1246450, \n",
    "<br>\n",
    "https://www.bioinf.jku.at/publications/older/2604.pdf,\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Long_short-term_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(x,hidden_state,cell,input_dim,hidden_size,scope):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        w = tf.get_variable(\"w\", shape=[4,input_dim,hidden_size],\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=True,\n",
    "                                    initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "        u = tf.get_variable(\"u\", shape=[4,hidden_size,hidden_size],\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=True,\n",
    "                            initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "        b = tf.get_variable(\"bias\", shape=[4,1,hidden_size],\n",
    "                    dtype=tf.float32,\n",
    "                    trainable=True,\n",
    "                    initializer=tf.zeros_initializer())\n",
    "        \n",
    "    input_gate = tf.nn.sigmoid( tf.matmul(x,w[0]) + tf.matmul(hidden_state,u[0]) + b[0])\n",
    "    forget_gate = tf.nn.sigmoid( tf.matmul(x,w[1]) + tf.matmul(hidden_state,u[1]) + b[1])\n",
    "    output_gate = tf.nn.sigmoid( tf.matmul(x,w[2]) + tf.matmul(hidden_state,u[2]) + b[2])\n",
    "    cell_ = tf.nn.tanh( tf.matmul(x,w[3]) + tf.matmul(hidden_state,u[3]) + b[3])\n",
    "    cell = forget_gate*cell + input_gate*cell_\n",
    "    hidden_state = output_gate*tf.tanh(cell)\n",
    "    \n",
    "    return hidden_state, cell\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Directional LSTM Encoder\n",
    "\n",
    "(https://maxwell.ict.griffith.edu.au/spl/publications/papers/ieeesp97_schuster.pdf)\n",
    "\n",
    "More Info: https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/\n",
    "\n",
    "Bi-directional LSTM encoder has a forward encoder and a backward encoder. The forward encoder encodes a text sequence from start to end, and the backward encoder encodes the text sequence from end to start.\n",
    "The final output is a combination (in this case, a concatenation) of the forward encoded text and the backward encoded text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0201 19:55:58.176841 16108 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "S = tf.shape(embd_text)[1] #text sequence length\n",
    "N = tf.shape(embd_text)[0] #batch_size\n",
    "\n",
    "i=0\n",
    "hidden=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "cell=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "hidden_forward=tf.TensorArray(size=S, dtype=tf.float32)\n",
    "\n",
    "#shape of embd_text: [N,S,embd_dim]\n",
    "embd_text_t = tf.transpose(embd_text,[1,0,2]) \n",
    "#current shape of embd_text: [S,N,embd_dim]\n",
    "\n",
    "def cond(i, hidden, cell, hidden_forward):\n",
    "    return i < S\n",
    "\n",
    "def body(i, hidden, cell, hidden_forward):\n",
    "    x = embd_text_t[i]\n",
    "    \n",
    "    hidden,cell = LSTM(x,hidden,cell,embd_dim,hidden_size,scope=\"forward_encoder\")\n",
    "    hidden_forward = hidden_forward.write(i, hidden)\n",
    "\n",
    "    return i+1, hidden, cell, hidden_forward\n",
    "\n",
    "_, _, _, hidden_forward = tf.while_loop(cond, body, [i, hidden, cell, hidden_forward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=S-1\n",
    "hidden=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "cell=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "hidden_backward=tf.TensorArray(size=S, dtype=tf.float32)\n",
    "\n",
    "def cond(i, hidden, cell, hidden_backward):\n",
    "    return i >= 0\n",
    "\n",
    "def body(i, hidden, cell, hidden_backward):\n",
    "    x = embd_text_t[i]\n",
    "    hidden,cell = LSTM(x,hidden,cell,embd_dim,hidden_size,scope=\"backward_encoder\")\n",
    "    hidden_backward = hidden_backward.write(i, hidden)\n",
    "\n",
    "    return i-1, hidden, cell, hidden_backward\n",
    "\n",
    "_, _, _, hidden_backward = tf.while_loop(cond, body, [i, hidden, cell, hidden_backward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Forward and Backward Encoder Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_forward = hidden_forward.stack()\n",
    "hidden_backward = hidden_backward.stack()\n",
    "\n",
    "encoder_states = tf.concat([hidden_forward,hidden_backward],axis=-1)\n",
    "encoder_states = tf.transpose(encoder_states,[1,0,2])\n",
    "\n",
    "encoder_states = dropout(encoder_states,rate=0.3,training=tf_train)\n",
    "\n",
    "final_encoded_state = dropout(tf.concat([hidden_forward[-1],hidden_backward[-1]],axis=-1),rate=0.3,training=tf_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of attention scoring function\n",
    "\n",
    "Given a sequence of encoder states ($H_s$) and the decoder hidden state ($H_t$) of current timestep $t$, the equation for computing attention score is:\n",
    "\n",
    "$$Score = (H_s.W_a).H_t^T $$\n",
    "\n",
    "($W_a$ = trainable parameters)\n",
    "\n",
    "(https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_score(encoder_states,decoder_hidden_state,scope=\"attention_score\"):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        Wa = tf.get_variable(\"Wa\", shape=[2*hidden_size,2*hidden_size],\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=True,\n",
    "                                    initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "    encoder_states = tf.reshape(encoder_states,[N*S,2*hidden_size])\n",
    "    \n",
    "    encoder_states = tf.reshape(tf.matmul(encoder_states,Wa),[N,S,2*hidden_size])\n",
    "    decoder_hidden_state = tf.reshape(decoder_hidden_state,[N,2*hidden_size,1])\n",
    "    \n",
    "    return tf.reshape(tf.matmul(encoder_states,decoder_hidden_state),[N,S])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Attention Function\n",
    "\n",
    "Based on: https://nlp.stanford.edu/pubs/emnlp15_attn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align(encoder_states, decoder_hidden_state,scope=\"attention\"):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        Wp = tf.get_variable(\"Wp\", shape=[2*hidden_size,128],\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=True,\n",
    "                                    initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "        Vp = tf.get_variable(\"Vp\", shape=[128,1],\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=True,\n",
    "                            initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "    positions = tf.cast(S-window_len,dtype=tf.float32) # Maximum valid attention window starting position\n",
    "    \n",
    "    # Predict attention window starting position \n",
    "    ps = positions*tf.nn.sigmoid(tf.matmul(tf.tanh(tf.matmul(decoder_hidden_state,Wp)),Vp))\n",
    "    # ps = (soft-)predicted starting position of attention window\n",
    "    pt = ps+D # pt = center of attention window where the whole window length is 2*D+1\n",
    "    pt = tf.reshape(pt,[N])\n",
    "    \n",
    "    i = 0\n",
    "    gaussian_position_based_scores = tf.TensorArray(size=S,dtype=tf.float32)\n",
    "    sigma = tf.constant(D/2,dtype=tf.float32)\n",
    "    \n",
    "    def cond(i,gaussian_position_based_scores):\n",
    "        \n",
    "        return i < S\n",
    "                      \n",
    "    def body(i,gaussian_position_based_scores):\n",
    "        \n",
    "        score = tf.exp(-((tf.square(tf.cast(i,tf.float32)-pt))/(2*tf.square(sigma)))) \n",
    "        # (equation (10) in https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)\n",
    "        gaussian_position_based_scores = gaussian_position_based_scores.write(i,score)\n",
    "            \n",
    "        return i+1,gaussian_position_based_scores\n",
    "                      \n",
    "    i,gaussian_position_based_scores = tf.while_loop(cond,body,[i,gaussian_position_based_scores])\n",
    "    \n",
    "    gaussian_position_based_scores = gaussian_position_based_scores.stack()\n",
    "    gaussian_position_based_scores = tf.transpose(gaussian_position_based_scores,[1,0])\n",
    "    gaussian_position_based_scores = tf.reshape(gaussian_position_based_scores,[N,S])\n",
    "    \n",
    "    scores = attention_score(encoder_states,decoder_hidden_state)*gaussian_position_based_scores\n",
    "    scores = tf.nn.softmax(scores,axis=-1)\n",
    "    \n",
    "    return tf.reshape(scores,[N,S,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Decoder With Local Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder\",reuse=tf.AUTO_REUSE):\n",
    "    SOS = tf.get_variable(\"sos\", shape=[1,embd_dim],\n",
    "                                dtype=tf.float32,\n",
    "                                trainable=True,\n",
    "                                initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "    # SOS represents starting marker \n",
    "    # It tells the decoder that it is about to decode the first word of the output\n",
    "    # I have set SOS as a trainable parameter\n",
    "    \n",
    "    Wc = tf.get_variable(\"Wc\", shape=[4*hidden_size,embd_dim],\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=True,\n",
    "                            initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "\n",
    "\n",
    "SOS = tf.tile(SOS,[N,1]) #now SOS shape: [N,embd_dim]\n",
    "inp = SOS\n",
    "hidden=final_encoded_state\n",
    "cell=tf.zeros([N, 2*hidden_size], dtype=tf.float32)\n",
    "decoder_outputs=tf.TensorArray(size=max_summary_len, dtype=tf.float32)\n",
    "outputs=tf.TensorArray(size=max_summary_len, dtype=tf.int32)\n",
    "\n",
    "attention_scores = align(encoder_states,hidden)\n",
    "encoder_context_vector = tf.reduce_sum(encoder_states*attention_scores,axis=1)\n",
    "\n",
    "for i in range(max_summary_len):\n",
    "    \n",
    "    inp = dropout(inp,rate=0.3,training=tf_train)\n",
    "    \n",
    "    inp = tf.concat([inp,encoder_context_vector],axis=-1)\n",
    "    \n",
    "    hidden,cell = LSTM(inp,hidden,cell,embd_dim+2*hidden_size,2*hidden_size,scope=\"decoder\")\n",
    "    \n",
    "    hidden = dropout(hidden,rate=0.3,training=tf_train)\n",
    "    \n",
    "    attention_scores = align(encoder_states,hidden)\n",
    "    encoder_context_vector = tf.reduce_sum(encoder_states*attention_scores,axis=1)\n",
    "    \n",
    "    concated = tf.concat([hidden,encoder_context_vector],axis=-1)\n",
    "    \n",
    "    linear_out = tf.nn.tanh(tf.matmul(concated,Wc))\n",
    "    decoder_output = tf.matmul(linear_out,tf.transpose(tf_embd,[1,0])) \n",
    "    # produce unnormalized probability distribution over vocabulary\n",
    "    \n",
    "    \n",
    "    decoder_outputs = decoder_outputs.write(i,decoder_output)\n",
    "    \n",
    "    # Pick out most probable vocab indices based on the unnormalized probability distribution\n",
    "    \n",
    "    next_word_vec = tf.cast(tf.argmax(decoder_output,1),tf.int32)\n",
    "\n",
    "    next_word_vec = tf.reshape(next_word_vec, [N])\n",
    "\n",
    "    outputs = outputs.write(i,next_word_vec)\n",
    "\n",
    "    next_word = tf.nn.embedding_lookup(tf_embd, next_word_vec)\n",
    "    inp = tf.reshape(next_word, [N, embd_dim])\n",
    "    \n",
    "    \n",
    "decoder_outputs = decoder_outputs.stack()\n",
    "outputs = outputs.stack()\n",
    "\n",
    "decoder_outputs = tf.transpose(decoder_outputs,[1,0,2])\n",
    "outputs = tf.transpose(outputs,[1,0])\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Cross Entropy Cost Function and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_trainables = [var for var in tf.trainable_variables() if\n",
    "                       not(\"Bias\" in var.name or \"bias\" in var.name\n",
    "                           or \"noreg\" in var.name)]\n",
    "\n",
    "regularization = tf.reduce_sum([tf.nn.l2_loss(var) for var\n",
    "                                in filtered_trainables])\n",
    "\n",
    "with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    epsilon = tf.constant(1e-9, tf.float32)\n",
    "\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=tf_summary, logits=decoder_outputs)\n",
    "\n",
    "    pad_mask = tf.sequence_mask(tf_true_summary_len,\n",
    "                                maxlen=max_summary_len,\n",
    "                                dtype=tf.float32)\n",
    "\n",
    "    masked_cross_entropy = cross_entropy*pad_mask\n",
    "\n",
    "    cost = tf.reduce_mean(masked_cross_entropy) + \\\n",
    "        l2*regularization\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(masked_cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0201 19:57:19.522096 16108 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Comparing predicted sequence with labels\n",
    "comparison = tf.cast(tf.equal(outputs, tf_summary),\n",
    "                     tf.float32)\n",
    "\n",
    "# Masking to ignore the effect of pads while calculating accuracy\n",
    "pad_mask = tf.sequence_mask(tf_true_summary_len,\n",
    "                            maxlen=max_summary_len,\n",
    "                            dtype=tf.bool)\n",
    "\n",
    "masked_comparison = tf.boolean_mask(comparison, pad_mask)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = tf.reduce_mean(masked_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "gvs = optimizer.compute_gradients(cost, all_vars)\n",
    "\n",
    "capped_gvs = [(tf.clip_by_norm(grad, 5), var) for grad, var in gvs] # Gradient Clipping\n",
    "\n",
    "train_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "STARTING TRAINING\n",
      "\n",
      "\n",
      "Iter 0, Cost= 1.651, Acc = 0.00%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "having a vague memory of seeing alphabet cookies somewhere , i sought them locally in vain & turned to amazon , where i found just what i wanted to give children from the <UNK> booth at our church ministry fair . i passed them out in little baggies of five each , happy to tell parents they were organic . as it turned out , the children loved them ; so did our senior warden and other adults who tried the delicious little goodies !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "flute thunderstorms cathedral manar expectable gills incense conchita conchita mauna noisily funnel jaws juniperus expectable peeled 6-3 6-3 6-3 6-3 jma expectable gills mfr expectable surround gills programing lighted mauna ringing \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "crowd pleasers ! \n",
      "\n",
      "\n",
      "Iter 100, Cost= 0.749, Acc = 25.00%\n",
      "Iter 200, Cost= 0.745, Acc = 26.72%\n",
      "Iter 300, Cost= 0.756, Acc = 25.56%\n",
      "Iter 400, Cost= 0.717, Acc = 24.24%\n",
      "Iter 500, Cost= 0.830, Acc = 25.35%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "excellent ! i use this in everything i cook , i use it in plain rice , fried rice , pasta , frying eggs anything . the flavoring is great and my house is always filled with the delicious aroma . will try using it in my oil burner and on my skin but its perfect for my hair and leaves it shiny and silky . i have been using this for a few months and i am only halfway through one jar so a little goes a long long way ! excellent product ! !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "i ! \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "s & s \n",
      "\n",
      "\n",
      "Iter 600, Cost= 0.887, Acc = 23.68%\n",
      "Iter 700, Cost= 0.841, Acc = 20.55%\n",
      "Iter 800, Cost= 0.673, Acc = 31.62%\n",
      "Iter 900, Cost= 0.902, Acc = 23.49%\n",
      "Iter 1000, Cost= 0.725, Acc = 31.85%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "great flavor ! my dog love it . i give her every day a mini bone as a reward for good behavior , and every day she wait happy ( excited ) for this award .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "highly <UNK> \n",
      "\n",
      "\n",
      "Iter 1100, Cost= 0.687, Acc = 29.46%\n",
      "Iter 1200, Cost= 0.735, Acc = 26.32%\n",
      "Iter 1300, Cost= 0.782, Acc = 22.06%\n",
      "Iter 1400, Cost= 0.764, Acc = 23.02%\n",
      "Iter 1500, Cost= 0.861, Acc = 24.31%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "my dogs love these treats . they are easy to break up and are a great incentive to obey . i have these delivered every month and it takes the pain out of running to the store .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "my dog \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "yum \n",
      "\n",
      "\n",
      "Iter 1600, Cost= 0.937, Acc = 23.03%\n",
      "Iter 1700, Cost= 0.743, Acc = 25.95%\n",
      "Iter 1800, Cost= 0.639, Acc = 27.27%\n",
      "Iter 1900, Cost= 0.818, Acc = 24.83%\n",
      "Iter 2000, Cost= 0.551, Acc = 34.45%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "perfect cup of green tea for every meal . i love green tea and this kind is perfect and handy to have ready when you need it .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great tea \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "great tea \n",
      "\n",
      "\n",
      "Iter 2100, Cost= 0.800, Acc = 24.83%\n",
      "Iter 2200, Cost= 0.769, Acc = 26.19%\n",
      "Iter 2300, Cost= 0.872, Acc = 24.20%\n",
      "Iter 2400, Cost= 0.807, Acc = 23.94%\n",
      "Iter 2500, Cost= 0.644, Acc = 27.41%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "it is n't easy to find really delicious cookies that are gluten-free , <UNK> , <UNK> , and do n't contain chocolate . these lemon wafers are all that -- truly yummy .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "delicious ! ! \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "yummy ! \n",
      "\n",
      "\n",
      "Iter 2600, Cost= 0.670, Acc = 32.84%\n",
      "Iter 2700, Cost= 0.797, Acc = 18.62%\n",
      "Iter 2800, Cost= 0.819, Acc = 24.49%\n",
      "Iter 2900, Cost= 0.891, Acc = 21.19%\n",
      "Iter 3000, Cost= 0.802, Acc = 24.66%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "this tea is very good . the flavor is great and it 's quite calming and soothing , especially in a nice bubble bath after a rough day !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great tea tea \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "<UNK> & soothing \n",
      "\n",
      "\n",
      "Iter 3100, Cost= 0.713, Acc = 26.95%\n",
      "Iter 3200, Cost= 0.688, Acc = 25.00%\n",
      "Iter 3300, Cost= 0.697, Acc = 26.72%\n",
      "Iter 3400, Cost= 0.711, Acc = 28.36%\n",
      "Iter 3500, Cost= 0.828, Acc = 22.30%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "this product was suddenly hard to find at the local <UNK> stores and i was glad to see i could get it at amazon . it arrived pretty fast with free shipping and is pretty fresh tasting with an expiration date well off into the next year . i 'm pretty happy .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "good \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "good price and free shipping \n",
      "\n",
      "\n",
      "Iter 3600, Cost= 0.748, Acc = 28.10%\n",
      "Iter 3700, Cost= 0.636, Acc = 30.47%\n",
      "Iter 3800, Cost= 0.755, Acc = 26.43%\n",
      "Iter 3900, Cost= 0.903, Acc = 21.79%\n",
      "Iter 4000, Cost= 0.751, Acc = 23.53%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i ordered these and they showed up stale . i guess that can happen but when i tried to return them amazon listed them as `` unreturnable '' . nothing in the ad indicates this so if you buy them , you are stuck with them . no matter what . this is the first item i have found , sold by amazon and listed under the prime label that wont let you return under any circumstances .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "not \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "a little misleading \n",
      "\n",
      "\n",
      "Iter 4100, Cost= 0.721, Acc = 26.39%\n",
      "Iter 4200, Cost= 0.614, Acc = 30.00%\n",
      "Iter 4300, Cost= 0.731, Acc = 26.28%\n",
      "Iter 4400, Cost= 0.804, Acc = 24.83%\n",
      "Iter 4500, Cost= 0.668, Acc = 29.37%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "this item is way over priced . do n't pay $ 1 / cup . you can get it around $ 0.60 / cup anywhere else online . if you are paying $ 1 / cup , you might as well go to dunkin donuts .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "price price \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "ridiculous price \n",
      "\n",
      "\n",
      "Iter 4600, Cost= 0.799, Acc = 24.11%\n",
      "Iter 4700, Cost= 0.691, Acc = 31.58%\n",
      "Iter 4800, Cost= 0.762, Acc = 25.53%\n",
      "Iter 4900, Cost= 0.572, Acc = 31.45%\n",
      "Iter 5000, Cost= 0.805, Acc = 22.45%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "just received my second or third order from `` no thyme productions '' , and same as the earlier shipments , it arrived before the expected date , everything included and in good shape . i most definitely recommend this company ! the cat 's love the mixed greens assortment .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "fast & perfect service ! \n",
      "\n",
      "\n",
      "Iter 5100, Cost= 0.815, Acc = 26.03%\n",
      "Iter 5200, Cost= 0.742, Acc = 28.12%\n",
      "Iter 5300, Cost= 0.734, Acc = 23.74%\n",
      "Iter 5400, Cost= 0.894, Acc = 23.97%\n",
      "Iter 5500, Cost= 0.807, Acc = 23.61%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "like the coffee . love the single cup system . delivery was fast and reliable as always . and really , it does have a nice coffee kick , but it wo n't give you the jitters .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "good coffee \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "not as strong as it sounds . \n",
      "\n",
      "\n",
      "Iter 5600, Cost= 0.789, Acc = 25.68%\n",
      "Iter 5700, Cost= 0.687, Acc = 30.88%\n",
      "Iter 5800, Cost= 0.765, Acc = 23.70%\n",
      "Iter 5900, Cost= 1.015, Acc = 24.07%\n",
      "Iter 6000, Cost= 0.763, Acc = 29.55%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "my dogs loves this treat and all the treats by <UNK> . they are made in the usa what more could you ask for . great service from amazon .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great dog \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "great product \n",
      "\n",
      "\n",
      "Iter 6100, Cost= 0.744, Acc = 26.03%\n",
      "Iter 6200, Cost= 0.739, Acc = 25.93%\n",
      "Iter 6300, Cost= 0.841, Acc = 25.17%\n",
      "Iter 6400, Cost= 0.894, Acc = 24.50%\n",
      "Iter 6500, Cost= 0.693, Acc = 25.00%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "the huckleberry jam was good , unforgettable , wonderful , and delicious . i have really enjoyed eating this jam . i will buy it again , and recommend it to others .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "great \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "huckleberry jam \n",
      "\n",
      "\n",
      "Iter 6600, Cost= 0.883, Acc = 23.08%\n",
      "Iter 6700, Cost= 0.684, Acc = 28.57%\n",
      "Iter 6800, Cost= 0.734, Acc = 27.89%\n",
      "Iter 6900, Cost= 0.641, Acc = 27.20%\n",
      "Iter 7000, Cost= 0.885, Acc = 21.95%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "although i did not buy these for myself , they are the same as the `` turkish delights '' i brought home from a trip which included part of turkey . therefore , i can assure anyone that they are a delicious , delightful sweet ( candy ) made with fruit juices and with <UNK> nuts in them . they have been given as christmas gifts , and i am sure the recipients will thoroughly enjoy them !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "delicious ! \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "a special treat ! \n",
      "\n",
      "\n",
      "Iter 7100, Cost= 0.680, Acc = 29.77%\n",
      "Iter 7200, Cost= 0.568, Acc = 32.23%\n",
      "Iter 7300, Cost= 0.742, Acc = 28.26%\n",
      "Iter 7400, Cost= 0.854, Acc = 23.38%\n",
      "Iter 7500, Cost= 0.726, Acc = 25.83%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "i have been in love with these cereals for a few years . since they do not carry them anymore in stores where i live , i was trying to find them somewhere else . i go to florida once a year and found them at a wal-mart store . i brought some back home , but i 've eaten them all ! ! ! thank you amazon ! ! ! ! !\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "my ! \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "florida \n",
      "\n",
      "\n",
      "Iter 7600, Cost= 0.700, Acc = 27.41%\n",
      "Iter 7700, Cost= 0.828, Acc = 24.84%\n",
      "Iter 7800, Cost= 0.907, Acc = 22.37%\n",
      "Iter 7900, Cost= 0.768, Acc = 26.06%\n",
      "Iter 8000, Cost= 0.732, Acc = 25.71%\n",
      "\n",
      "Sample Text\n",
      "\n",
      "this tea is an excellent product . i am not a health expert but the tea seems to help rid the body of excess fat .\n",
      "\n",
      "Sample Predicted Summary\n",
      "\n",
      "good tea \n",
      "\n",
      "Sample Actual Summary\n",
      "\n",
      "<UNK> source premium blend tea \n",
      "\n",
      "\n",
      "Iter 8100, Cost= 0.735, Acc = 28.57%\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "with tf.Session() as sess:  # Start Tensorflow Session\n",
    "    display_step = 100\n",
    "    patience = 5\n",
    "\n",
    "    load = input(\"\\nLoad checkpoint? y/n: \")\n",
    "    print(\"\")\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    if load.lower() == 'y':\n",
    "\n",
    "        print('Loading pre-trained weights for the model...')\n",
    "\n",
    "        saver.restore(sess, 'Model_Backup/Seq2seq_summarization.ckpt')\n",
    "        sess.run(tf.global_variables())\n",
    "        sess.run(tf.tables_initializer())\n",
    "\n",
    "        with open('Model_Backup/Seq2seq_summarization.pkl', 'rb') as fp:\n",
    "            train_data = pickle.load(fp)\n",
    "\n",
    "        covered_epochs = train_data['covered_epochs']\n",
    "        best_loss = train_data['best_loss']\n",
    "        impatience = 0\n",
    "        \n",
    "        print('\\nRESTORATION COMPLETE\\n')\n",
    "\n",
    "    else:\n",
    "        best_loss = 2**30\n",
    "        impatience = 0\n",
    "        covered_epochs = 0\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        sess.run(tf.tables_initializer())\n",
    "\n",
    "    epoch=0\n",
    "    while (epoch+covered_epochs)<epochs:\n",
    "        \n",
    "        print(\"\\n\\nSTARTING TRAINING\\n\\n\")\n",
    "        \n",
    "        batches_indices = [i for i in range(0, len(train_batches_text))]\n",
    "        random.shuffle(batches_indices)\n",
    "\n",
    "        total_train_acc = 0\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for i in range(0, len(train_batches_text)):\n",
    "            \n",
    "            j = int(batches_indices[i])\n",
    "\n",
    "            cost,prediction,\\\n",
    "                acc, _ = sess.run([cross_entropy,\n",
    "                                   outputs,\n",
    "                                   accuracy,\n",
    "                                   train_op],\n",
    "                                  feed_dict={tf_text: train_batches_text[j],\n",
    "                                             tf_embd: embd,\n",
    "                                             tf_summary: train_batches_summary[j],\n",
    "                                             tf_true_summary_len: train_batches_true_summary_len[j],\n",
    "                                             tf_train: True})\n",
    "            \n",
    "            total_train_acc += acc\n",
    "            total_train_loss += cost\n",
    "\n",
    "            if i % display_step == 0:\n",
    "                print(\"Iter \"+str(i)+\", Cost= \" +\n",
    "                      \"{:.3f}\".format(cost)+\", Acc = \" +\n",
    "                      \"{:.2f}%\".format(acc*100))\n",
    "            \n",
    "            if i % 500 == 0:\n",
    "                \n",
    "                idx = random.randint(0,len(train_batches_text[j])-1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                text = \" \".join([idx2vocab.get(vec,\"<UNK>\") for vec in train_batches_text[j][idx]])\n",
    "                predicted_summary = [idx2vocab.get(vec,\"<UNK>\") for vec in prediction[idx]]\n",
    "                actual_summary = [idx2vocab.get(vec,\"<UNK>\") for vec in train_batches_summary[j][idx]]\n",
    "                \n",
    "                print(\"\\nSample Text\\n\")\n",
    "                print(text)\n",
    "                print(\"\\nSample Predicted Summary\\n\")\n",
    "                for word in predicted_summary:\n",
    "                    if word == '<EOS>':\n",
    "                        break\n",
    "                    else:\n",
    "                        print(word,end=\" \")\n",
    "                print(\"\\n\\nSample Actual Summary\\n\")\n",
    "                for word in actual_summary:\n",
    "                    if word == '<EOS>':\n",
    "                        break\n",
    "                    else:\n",
    "                        print(word,end=\" \")\n",
    "                print(\"\\n\\n\")\n",
    "                \n",
    "        print(\"\\n\\nSTARTING VALIDATION\\n\\n\")\n",
    "                \n",
    "        total_val_loss=0\n",
    "        total_val_acc=0\n",
    "                \n",
    "        for i in range(0, len(val_batches_text)):\n",
    "            \n",
    "            if i%100==0:\n",
    "                print(\"Validating data # {}\".format(i))\n",
    "\n",
    "            cost, prediction,\\\n",
    "                acc = sess.run([cross_entropy,\n",
    "                                outputs,\n",
    "                                accuracy],\n",
    "                                  feed_dict={tf_text: val_batches_text[i],\n",
    "                                             tf_embd: embd,\n",
    "                                             tf_summary: val_batches_summary[i],\n",
    "                                             tf_true_summary_len: val_batches_true_summary_len[i],\n",
    "                                             tf_train: False})\n",
    "            \n",
    "            total_val_loss += cost\n",
    "            total_val_acc += acc\n",
    "            \n",
    "        avg_val_loss = total_val_loss/len(val_batches_text)\n",
    "        \n",
    "        print(\"\\n\\nEpoch: {}\\n\\n\".format(epoch+covered_epochs))\n",
    "        print(\"Average Training Loss: {:.3f}\".format(total_train_loss/len(train_batches_text)))\n",
    "        print(\"Average Training Accuracy: {:.2f}\".format(100*total_train_acc/len(train_batches_text)))\n",
    "        print(\"Average Validation Loss: {:.3f}\".format(avg_val_loss))\n",
    "        print(\"Average Validation Accuracy: {:.2f}\".format(100*total_val_acc/len(val_batches_text)))\n",
    "              \n",
    "        if (avg_val_loss < best_loss):\n",
    "            best_loss = avg_val_loss\n",
    "            save_data={'best_loss':best_loss,'covered_epochs':covered_epochs+epoch+1}\n",
    "            impatience=0\n",
    "            with open('Model_Backup/Seq2seq_summarization.pkl', 'wb') as fp:\n",
    "                pickle.dump(save_data, fp)\n",
    "            saver.save(sess, 'Model_Backup/Seq2seq_summarization.ckpt')\n",
    "            print(\"\\nModel saved\\n\")\n",
    "              \n",
    "        else:\n",
    "            impatience+=1\n",
    "              \n",
    "        if impatience > patience:\n",
    "              break\n",
    "              \n",
    "              \n",
    "        epoch+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Works\n",
    "\n",
    "* Beam Search\n",
    "* Pointer Mechanisms\n",
    "* BLEU\\ROUGE evaluation\n",
    "* Implement Testing\n",
    "* Complete Training and Optimize Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
